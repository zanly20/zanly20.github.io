<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Li Zhang, Li Zhang, Li Zhang, Li Zhang, Li Zhang, Li Zhang, ustc,  University of Science and Technology of China">
<meta name="description" content="Li Zhang">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Li Zhang's homepage, USTC</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
	
<nav class="navbar navbar-dark navbar-expand-lg fixed-top">
    <div id="layout-menu">
        <a href="#">Home</a>
        <a href="#biography">Biography</a>
		<a href="#news">News</a>
		<a href="#researches">Researches</a>
        <a href="#publications">Publications</a>
<a href="#work">Work</a>
        <a href="#service">Service</a>
    </div>
</nav>
	
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Arial"> Li Zhang </font></h1>
					<h1><font face="Arial"> Âº†Âäõ </font></h1>
					 <!-- <h1><font face="Arial"> Âº†Âäõ </font></h1> -->
				</div>

				<h3><font face="Arial"> Ph.D Candidate </font></h3>
				<p><font face="Arial"> 
					University of Science and Technology of China, Anhui, China <br>
					
					<br>
					<em>Email: <a href="mailto:xzyr@mail.ustc.edu.cn">zanly20@mail.ustc.edu.cn</a></em> <br>
					
					<!-- <a href="https://xw-hu.github.io/XiaoweiHu.pdf"><em>[Curriculum Vitae]</em></a>
					&nbsp;&nbsp
					-->
				</font></p>
				<!--<p> <a href="https://scholar.google.com/citations?user=tUb4J0kAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/xw-hu"><img src="./pic/github_s.jpg" height="20px" style="margin-bottom:-3px"></a>
					<a href="https://www.facebook.com/xiaowei.hu.102"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p> -->
			</td>
			<td>
				<img src="./images/own/QR_code.png" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>



	
<div id="biography">
<h2><font face="Arial"> Biography </font></h2>
<p style="text-align:justify"><font face="Arial">
	
	I am currently a PhD student in <a href="https://www.ustc.edu.cn/">University of Science and Technology of China (USTC) </a>,
	advised by Prof. <a href="https://auto.ustc.edu.cn/2021/0510/c25976a484866/page.htm">Rujing Wang</a>. <br>
<!--    and Prof. <a href="https://sai.sjtu.edu.cn/cn/facultydetails/zzjs/lucewu">Cewu Lu</a> -->
	I collaborate closely with Prof. <a href="https://faculty.hfut.edu.cn/liuliu/en/index.htm">Liu Liu</a>
	and Prof. <a href="https://scholar.google.com/citations?user=08U8joq2FOQC&hl=zh-CN&oi=sra">Jiankun Wang</a>.
	My research interests focus on computer vision and embodied AI, especially <strong>6D pose estimation</strong> and <strong>Manipulation</strong>.
	I actively collaborate with leading industry partners such as <a href="https://ailab.tencent.com">Tencent AI Lab</a>, <a href="https://www.iflytek.com">IFLYTEK</a>, <a href="https://www.astribot.com/">Astribot</a>,
	focusing on cutting-edge research in pose perception.
	I am a strong advocate of <strong>sharingüå±, collaboratingü§ù, advancingüöÄ, and innovatingüí°</strong>.
	Passionate about bridging fundamental research with real-world impact, I strive to ensure my work contributes to both academic progress and practical value.</p>

	<p><em><strong>If you‚Äôre interested in collaborating or would like to connect, feel free to reach out via email (ÈÇÆÁÆ±Ôºözanly20@mail.ustc.edu.cn) or Wechat  (ÂæÆ‰ø°Âè∑Ôºözanly20)!</strong></em> üòä</p>




<div id="news">
<h2><font face="Arial"> News </font></h2>
<p style="text-align:justify"><font face="Arial">
<ul>
  <li><em>2025.07</em>:&nbsp;ü•≥ü•≥Two paper is accepted by ACM MM 2025! See you in Dublin!</li>
  <li><em>2025.04</em>:&nbsp;üéâüéâOne paper is accepted by IJCAI 2025! See you in GuangZhou!</li>
  <li><em>2025.02</em>:&nbsp;ü•≥ü•≥One paper is accepted by CVPR2025 See you in Nashville TN!</li>
  <li><em>2025.01</em>:&nbsp;üéâüéâOne paper is accepted by IEEE Transactions on Emerging Topics in Computational Intelligence (T-ETCI).</li>
  <li><em>2024.12</em>: ü•≥ü•≥One paper is accepted by AAAI2025, See you in Pennsylvania!</li>
</ul>


<div id="researches">
<h2><font face="Arial">Researches </font></h2>
<p style="text-align:justify"><font face="Arial">

	My current research focuses on the visual perception of articulated and flexible objects,
	aiming to develop advanced 3D pose estimation methods to empower robots with fine manipulation and dexterous grasping capabilities.
	While significant progress has been made in enabling robotic operation in idealized settings,
	challenges related to robustness and adaptability persist in complex and unstructured environments such as underwater exploration, medical intervention, and industrial inspection.
	In response to the unique perception demands posed by these scenarios, I am actively expanding my research into new directions,
	with an emphasis on critical visual perception problems and corresponding solutions for underwater and medical robotics.

	
<div id="publications">
<h2><font face="Arial">Selected Publications </font>  <font face="Arial" size="3"><a href="https://scholar.google.com/citations?user=r5K6_ysAAAAJ&hl=zh-CN&oi=sra">[Google Scholar]</a></font> </h2>


<p><font face="Arial" size="4"><b>CVPR 2025</b></font></p>

	<div align="center">
  <img src="./images/own/CVPR2025.png" width="700"/>
</div>

<ul>
  <li> <strong><em>Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction</em></strong>.</li>
</ul>

<p><strong>L Zhang</strong>, M Xu, J Wang, Q Yu, L Yang, Y Li, C Lu, R Wang, L Liu</p>

<p>
  <a href="">[Paper]</a>
  <a href="">[BibTeX]</a>
  <a href="https://sites.google.com/view/gapt-dar">[Project/Codes]</a>
</p>

<p><span style="color:red"><strong>[TL;DR:]</strong></span> <em>2D Virtual try-on methods can be introduced into the garment pose perception.</em></p>


<p><font face="Arial" size="4"><b>AAAI 2025</b></font></p>
<div align="center">
  <img src="./images/own/AAAI2025.png" width="700"/>
</div>

<ul>
  <li> <strong><em>R2-Art: Category-level Articulation Pose Estimation from Single RGB Image via Cascade Render Strategy</em></strong>.</li>
</ul>

<p><strong>L Zhang</strong>, H Jiang, Y Huo, Y Zhong, J Wang, X Wang, R Wang, L Liu</p>

<p>
  <a href="">[Paper]</a>
  <a href="">[BibTeX]</a>
  <a href="">[Project/Codes]</a>
</p>

<p><span style="color:red"><strong>[TL;DR:]</strong></span> <em>6D pose estimation without depth information.</em></p>


<p><font face="Arial" size="4"><b>BMVC 2024</b></font></p>
<div align="center">
  <img src="./images/own/BMVC.png" width="700"/>
</div>

<ul>
  <li><strong><em>ICAF-4: An Integrated Framework of Category-level Articulated Object Perception and Manipulation for Embodied Intelligence</em></strong>.</li>
</ul>

<p>W Xu, <strong>L Zhang</strong>, Q Li, Q Wu, L Wu, L Liu</p>

<p>
  <a href="">[Paper]</a>
  <a href="">[BibTeX]</a>
  <a href="https://github.com/xwb0117/ICAF-4">[Project/Codes]</a>
</p>

<p><span style="color:red"><strong>[TL;DR:]</strong></span> <em>An novel Integrated Framework for four mainstream articulation tasks.</em></p>


	<p><font face="Arial" size="4"><b>ACM MM 2024</b></font></p>
<div align="center">
  <img src="./images/own/MM24.png" width="700"/>
</div>

<ul>
  <li><strong><em>VoCAPTER: Voting-based Pose Tracking for Category-level Articulated Object via Inter-frame Priors</em></strong>.</li>
</ul>

<p><strong>L Zhang</strong>, Z Han, Y Zhong, Q Yu, X Wu, X Wang, R Wang</p>

<p>
  <a href="">[Paper]</a>
  <a href="">[BibTeX]</a>
  <a href="https://github.com/zanly20">[Project/Codes]</a>
</p>

<p><span style="color:red"><strong>[TL;DR:]</strong></span> <em>What serves as the bridge between pose estimation and pose tracking? Answer: inter-frame priors!</em></p>


	<p><font face="Arial" size="4"><b>NeurIPS 2024(spotlight)</b></font></p>
<div align="center">
  <img src="./images/own/nips24.png" width="700"/>
</div>

<ul>
  <li> <strong><em>Rethinking 3D Convolution in &ell;<sub>p</sub>-norm Space</em></strong>.</li>
</ul>

<p><strong>L Zhang</strong>, Y Zhong, J Wang, Z Min, R Wang, L Liu</p>

<p>
  <a href="">[Paper]</a>
  <a href="">[BibTeX]</a>
  <a href="https://github.com/zanly20">[Project/Codes]</a>
</p>

<p><span style="color:red"><strong>[TL;DR:]</strong></span> <em>Does 3D Convolution really need inner production? We made an exploration on &ell;<sub>p</sub>-norm convolution.</em></p>




	
	
<div id="work">
<h2><font face="Arial"> Work/Intern Experiences </h2>
	
<ul style="list-style-type:none">
<p style="line-height: 120%; margin-left: 0px; margin-top: 8pt; margin-bottom: 8pt;"> <font face="Arial" size="3">

	<ul>
  <li>
    <em>2024.01 - 2025.01</em>, Astribot, Shenzhen, China. Topic: 6D pose estimation.
    (Supervisor: <a href="https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=zh-CN&oi=sra">Dr. Jianan Wang</a>)
  </li>

  <li>
    <em>2021.03 - 2021.10</em>, Tencent YouTu Lab, China. Topic: scene text localization, self-supervised learning.
    (Supervisor: Dr. Jason Liu, Deqiang Jiang)
  </li>

  <li>
    <em>2020.11 - 2021.02</em>, IFlytek AI Research Institute, China. Topic: deep learning.
    (Supervisor: Dr. Liang Sun)
  </li>
</ul>

</p>
</ul>

<div id="services">
<h2><font face="Arial">Services </font></h2>
<p style="text-align:justify"><font face="Arial">


	<p><font face="Arial" size="4"><b>Journal Reviewer</b></font></p>
<ul>
  <li>
    <strong>IEEE</strong>: IEEE Transactions on Pattern Analysis and Machine Intelligence (CCF-A),
    IEEE Transactions on Image Processing (CCF-A), IEEE Transactions on Mobile Computing (CCF-A),
    IEEE Transactions on Information Forensics and Security (CCF-A),
    IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Fuzzy Systems,
    IEEE Transactions on Cybernetics, IEEE Transactions on Multimedia,
    IEEE Transactions on Computational Social Systems, IEEE Internet of Things Journal,
    IEEE Transactions on Circuits and Systems for Video Technology,
    IEEE Transactions on Emerging Topics in Computational Intelligence, etc.
  </li>
  <li>
    <strong>ACM</strong>: ACM Transactions on Information Systems (CCF-A),
    ACM Transactions on Software Engineering and Methodology (CCF-A),
    ACM Transactions on Knowledge Discovery from Data,
    ACM Transactions on Intelligent Systems and Technology, etc.
  </li>
  <li>
    <strong>Springer</strong>: International Journal of Computer Vision (CCF-A),
    Cognitive Computation, etc.
  </li>
  <li>
    <strong>Elsevier</strong>: Artificial Intelligence (CCF-A),
    Information Fusion, Information Processing and Management,
    Neural Networks, Knowledge-Based Systems, Neurocomputing, etc.
  </li>
</ul>


	<p><font face="Arial" size="4"><b>Conference Service</b></font></p>

<ul>
  <li><strong>Area Chair</strong>: IJCNN'25.</li>
  <li>
    <strong>Program Committee Member</strong>: CVPR23-25, ECCV24, ICCV23/25, AAAI22-25,
    ACM MM22-25, NIPS24-25, ICLR24, etc.
  </li>
</ul>

		


</body></html>

